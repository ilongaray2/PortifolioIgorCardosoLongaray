{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula tempo do workflow\n",
    "def calculate_time_wf(inicio, fim):    \n",
    "    delta = fim - inicio\n",
    "    return int(delta.seconds/60)\n",
    "\n",
    "# calcula idade\n",
    "def calculate_age(born):\n",
    "    today = date.today()\n",
    "    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "\n",
    "# cria dummies de periodo do dia\n",
    "def set_shift(x):\n",
    "    if 0<x<=6:\n",
    "        return 'madrugada'\n",
    "    elif 6<x<=11:\n",
    "        return 'manha'\n",
    "    elif 11<x<=14:\n",
    "        return 'almoco'\n",
    "    elif 14<x<=17:\n",
    "        return 'tarde'\n",
    "    elif 17<x<=19:\n",
    "        return 'janta'\n",
    "    else:\n",
    "        return 'noite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cff97620bc11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     dfaux=pd.read_csv('/home/dev/poc/arquivos/model/in/Query_Multisat_'+str(j)+'.txt',sep='\\t',header=0,\n\u001b[0m\u001b[0;32m     10\u001b[0m                     names=['nrviagem','placa','cidadeorigem','estadoorigem',\n\u001b[0;32m     11\u001b[0m                            \u001b[1;34m'cidadedestino'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'estadodestino'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'vlrtotal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "mes=['Jun','Jul','Ago','Set','Out','Nov','Dez','Jan','Fev', 'Mar','Abr', 'Jun2019', 'Jul2019']\n",
    "#mes=['Fev']\n",
    "\n",
    "#versao='v2.0'\n",
    "\n",
    "listMultisat = []\n",
    "\n",
    "for i,j in enumerate(mes):\n",
    "    dfaux=pd.read_csv('/home/dev/poc/arquivos/model/in/Query_Multisat_'+str(j)+'.txt',sep='\\t',header=0,\n",
    "                    names=['nrviagem','placa','cidadeorigem','estadoorigem',\n",
    "                           'cidadedestino','estadodestino','vlrtotal',\n",
    "                           'tpoperacao','tpocorrencia','urbana','prioridade',\n",
    "                           'dtiniciowf','dtfimwf','nomemotorista',\n",
    "                           'cpfmotorista','tpcarga','tptecnologia',\n",
    "                           'cto'],error_bad_lines=False,low_memory=False)\n",
    "    listMultisat.append(dfaux)\n",
    "    \n",
    "df = pd.concat(listMultisat, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tratar tipos de dados\n",
    "df['dtiniciowf'] = pd.to_datetime(df['dtiniciowf'])\n",
    "df['dtfimwf'] = pd.to_datetime(df['dtfimwf'], errors='coerce').fillna(pd.to_datetime(df['dtiniciowf']) + timedelta(minutes=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cpfmotorista'] = pd.to_numeric(df['cpfmotorista'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupar por viagem e totalizando por tecnologia\n",
    "df1 = df.drop(['placa','cidadeorigem','estadoorigem','cidadedestino',\n",
    "               'estadodestino','vlrtotal','tpoperacao','tpocorrencia',\n",
    "               'urbana','prioridade','dtiniciowf','dtfimwf','nomemotorista',\n",
    "               'cpfmotorista','tpcarga','cto'], axis=1)\n",
    "\n",
    "tecnologias=['ONX','OMN','SAS','SGH','TRC','LOC','ATC','OUTRAS']\n",
    "\n",
    "# valida tipos de tecnologias válidas - caso identifique alguma diferente classifica como OUTRAS\n",
    "df1.loc[df1['tptecnologia'].isin(tecnologias)==False, 'tptecnologia'] = 'OUTRAS'\n",
    "\n",
    "# transforma linhas em colunas por tipo de tecnologia\n",
    "df1 = pd.get_dummies(df1, prefix=['t_'], columns=['tptecnologia'])\n",
    "\n",
    "# criar colunas que estejam faltando\n",
    "tecnologias = ['t__{0}'.format(t) for t in tecnologias]\n",
    "\n",
    "for t in tecnologias:\n",
    "    if t not in df1.columns:\n",
    "        df[t] = 0\n",
    "\n",
    "df1 = df1.groupby(['nrviagem'], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupar por viagem e totalizando por prioridade\n",
    "df2 = df.drop(['placa','cidadeorigem','estadoorigem','cidadedestino',\n",
    "               'estadodestino','vlrtotal','tpoperacao','tpocorrencia',\n",
    "               'urbana','dtiniciowf','dtfimwf','nomemotorista',\n",
    "               'cpfmotorista','tpcarga','tptecnologia','cto'], axis=1)\n",
    "\n",
    "prioridades=[1,2,3,4,5,6,7,9,10,11,12,13,15,18,19,20,95,96,97,98,99]\n",
    "\n",
    "# valida as prioridades válidas\n",
    "df2.loc[df2['prioridade'].isin(prioridades)==False, 'prioridade'] = 0\n",
    "\n",
    "# transforma linhas em colunas por prioridade\n",
    "df2 = pd.get_dummies(df2, prefix=['p_'], columns=['prioridade'])\n",
    "\n",
    "# remover colunas com prioridade = 0 (p__0)\n",
    "if 'p__0' in df.columns:\n",
    "    del df2['p__0']\n",
    "\n",
    "# criar colunas que estejam faltando\n",
    "prioridades = ['p__{0}'.format(p) for p in prioridades]\n",
    "\n",
    "for p in prioridades:\n",
    "    if p not in df.columns:\n",
    "        df2[p] = 0\n",
    "\n",
    "df2 = df2.groupby(['nrviagem'], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupar por viagem e tratar a ocorrencia de duplicidades de data de inicio e cpfs \n",
    "df3 = df.drop(['placa','cidadeorigem','estadoorigem','cidadedestino','estadodestino',\n",
    "               'vlrtotal','tpoperacao','tpocorrencia','urbana','prioridade',\n",
    "               'dtfimwf','nomemotorista','tpcarga',\n",
    "               'tptecnologia'], axis=1)\n",
    "\n",
    "df3 = df3.groupby(['nrviagem'], as_index=False).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('/home/dev/poc/arquivos/model/in/Query_Motoristas.txt', sep='\\t', \n",
    "                 names=['nomemotorista','endereco','nomepai',\n",
    "                        'nomemae','dtnasc', 'rg', 'cpfmotorista',\n",
    "                        'orgaoemissor','uf','dtexpedicaorg','sexo',\n",
    "                        'estadocivil','nrprocessosinistro','possuisinistro',\n",
    "                        'tiposinistro','dtunknow','ultimaempresa'],\n",
    "                 error_bad_lines=False)\n",
    "\n",
    "df5 = df5.drop(['nomemotorista','endereco','nomepai','nomemae',\n",
    "                'rg','orgaoemissor','uf','dtexpedicaorg','sexo',\n",
    "                'nrprocessosinistro','tiposinistro','dtunknow',\n",
    "                'ultimaempresa'], axis=1)\n",
    "\n",
    "# trata dados de motorista\n",
    "df5['cpfmotorista'] = pd.to_numeric(df5['cpfmotorista'],errors='coerce')\n",
    "df5['dtnasc'] = pd.to_datetime(df5['dtnasc'],errors='coerce')\n",
    "df5['idade'] = df5['dtnasc'].apply(calculate_age)\n",
    "\n",
    "# trata problema referente a dados repetidos\n",
    "df5 = df5.groupby(['cpfmotorista'], as_index=False).max()\n",
    "df5.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# une dados de ocorrencia com dados de motorista pelo CPF\n",
    "df5 = pd.merge(df3, df5, on='cpfmotorista', how='left')\n",
    "\n",
    "del(df3)\n",
    "\n",
    "# trata dados com NA\n",
    "df5.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria variaveis adicionais\n",
    "df['diasemanawf'] = df['dtiniciowf'].apply( lambda x: x.dayofweek) # 0 - Segunda até 6 - Domingo\n",
    "df['nrdiasiniciomeswf'] = df['dtiniciowf'].apply( lambda x: x.day) # nr dias desde o inicio do mês\n",
    "df['turno']=df['dtiniciowf'].dt.hour\n",
    "df['turno']=df['turno'].apply(set_shift)\n",
    "\n",
    "\n",
    "turnos=['almoco','janta','madrugada','manha','noite','tarde']\n",
    "\n",
    "# transforma linhas em colunas por mercadoria\n",
    "df = pd.get_dummies(df, prefix=['s'],prefix_sep=[''],columns=['turno'])\n",
    "\n",
    "# criar colunas que estejam faltando\n",
    "turnos = ['s{0}'.format(t) for t in turnos]\n",
    "\n",
    "for t in turnos:\n",
    "    if t not in df.columns:\n",
    "        df[t] = 0\n",
    "\n",
    "df = df.drop(['cpfmotorista','nomemotorista','tptecnologia','prioridade','cto'],axis=1)\n",
    "df = pd.merge(df, df5, on=['nrviagem','dtiniciowf'], how='inner')\n",
    "df = pd.merge(df, df1, on=['nrviagem'], how='inner')\n",
    "df = pd.merge(df, df2, on=['nrviagem'], how='inner')\n",
    "\n",
    "del(df5)\n",
    "del(df1)\n",
    "del(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tempowf'] = df.apply(lambda row: calculate_time_wf(row['dtiniciowf'], row['dtfimwf']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.reset_index().sort_values(by=['tempowf'], ascending=False)[['nrviagem','tpocorrencia', 'tempowf','dtiniciowf', 'dtfimwf']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elimina registros duplicados\n",
    "df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando as cidades de destino\n",
    "#cidades=df['cidadedestino'].value_counts()\n",
    "#pct = (cidades/cidades.sum())*100\n",
    "#cum=pct.cumsum()\n",
    "#cidades=pd.concat([cidades,pct,cum],axis=1,keys=['total','perct.','cumulative'])\n",
    "\n",
    "# seleciona aqui o percentual acumulado de interesse\n",
    "#cidades=cidades[cidades['cumulative']<70].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando as cidades de destino\n",
    "#cidades=df['cidadedestino'].value_counts()\n",
    "#pct = (cidades/cidades.sum())*100\n",
    "#cum=pct.cumsum()\n",
    "#cidades=pd.concat([cidades,pct,cum],axis=1,keys=['total','perct.','cumulative'])\n",
    "\n",
    "# seleciona aqui o percentual acumulado de interesse\n",
    "# cidades=cidades[cidades['cumulative']<70].index.tolist()\n",
    "\n",
    "cidades=['ANAPOLIS','APARECIDA DE GOIANIA','ARAUCARIA','BARRA MANSA','BARUERI','BAURU','BELEM',\n",
    "         'BELO HORIZONTE','BETIM','BLUMENAU','BRASILIA','CAMPINAS','CANOAS','CARAZINHO',\n",
    "         'CAXIAS DO SUL','CONTAGEM','CUBATAO','CUIABA','CURITIBA','DUQUE DE CAXIAS','EMBU DAS ARTES',\n",
    "         'FEIRA DE SANTANA','FORTALEZA','GASPAR','GOIANIA','GUARULHOS','HORTOLANDIA',\n",
    "         'ITAJAI','JABOATAO DOS GUARARAPES','JOINVILLE','JUNDIAI','LONDRINA','LOUVEIRA',\n",
    "         'MANAUS','NAVEGANTES','NOVA SANTA RITA','OSASCO','OUTRAS','PARANAGUA','PAULINIA','PORTO ALEGRE',\n",
    "         'RIBEIRAO PRETO','RIO DE JANEIRO','SALVADOR','SAO BERNARDO DO CAMPO','SAO JOSE DOS PINHAIS',\n",
    "         'SAO PAULO','SERRA','SIMOES FILHO','SUMARE','UBERLANDIA','VIANA','VILA VELHA',\n",
    "         'VITORIA DE SANTO ANTAO']\n",
    "\n",
    "# valida cidades válidas - caso identifique alguma diferente classifica como OUTRAS\n",
    "df.loc[df['cidadedestino'].isin(cidades)==False, 'cidadedestino'] = 'OUTRAS'\n",
    "\n",
    "# transforma linhas em colunas por cidade\n",
    "df = pd.get_dummies(df, prefix=['c_'], columns=['cidadedestino'])\n",
    "\n",
    "# criar colunas que estejam faltando\n",
    "cidades = ['c__{0}'.format(c) for c in cidades]\n",
    "\n",
    "for c in cidades:\n",
    "    if c not in df.columns:\n",
    "        df[c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando as mercadorias transportadas\n",
    "#cargas=df['tpcarga'].value_counts()\n",
    "#pct= (cargas/cargas.sum())*100\n",
    "#cum=pct.cumsum()\n",
    "#cargas=pd.concat([cargas,pct,cum],axis=1,keys=['total','perct.','cumulative'])\n",
    "\n",
    "# seleciona aqui o percentual acumulado de interesse\n",
    "#cargas=cargas[cargas['cumulative']<99.7].index.tolist()\n",
    "\n",
    "mercadorias=['ACUCAR','AUTO-PECAS','BEBIDAS','CALCADOS','CARGA FRACIONADA','CIGARROS','COMBUSTIVEL',\n",
    "             'COSMETICOS','DEFENSIVO AGRICOLA','DIVERSOS','DIVERSOS PRODUTOS','EQUIPAMENTOS ELETRICOS',\n",
    "             'ESPECIFICOS','ESTIRENO','FIOS E CABOS','MEDICAMENTOS','NESTLE','OUTRAS','PECAS AUTOMOTIVAS',\n",
    "             'POLIESTIRENO','POLIETILENO','PRODUTO ACABADO','PRODUTOS QUIMICOS','TINTAS E SOLVENTES']\n",
    "\n",
    "# classifica como OUTRAS as demais mercadorias que nao fiquem dentro do percentual de interesse\n",
    "df.loc[df['tpcarga'].isin(mercadorias)==False, 'tpcarga'] = 'OUTRAS'\n",
    "\n",
    "# converte as linhas para colunas por mercadorias\n",
    "df = pd.get_dummies(df, prefix=['m_'], columns=['tpcarga'])\n",
    "\n",
    "# criar colunas que estejam faltando\n",
    "mercadorias = ['m__{0}'.format(m) for m in mercadorias]\n",
    "\n",
    "for m in mercadorias:\n",
    "    if m not in df.columns:\n",
    "        df[m] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 223: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 87: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 27: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 34: expected 16 fields, saw 17\\nSkipping line 38: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 23: expected 16 fields, saw 17\\nSkipping line 72: expected 16 fields, saw 17\\nSkipping line 100: expected 16 fields, saw 17\\n'\n"
     ]
    }
   ],
   "source": [
    "# leitura do arquivo de sinistros (reguladora)\n",
    "#mes=['Jan']\n",
    "#mes=['Jun','Jul','Ago','Set','Out','Nov','Dez','Jan','Fev']\n",
    "\n",
    "listSinistros = []\n",
    "\n",
    "for i,j in enumerate(mes):\n",
    "    dfaux = pd.read_csv('/home/dev/poc/arquivos/model/in/Query_Sinistros_'+str(j)+'.txt', sep='\\t', header=0,\n",
    "                 names=['cdprocesso', 'nrprocesso', 'mercadoria', 'nrviagem', \n",
    "                        'tpsinistro', 'tpoperacao', 'txtunknow', \n",
    "                        'vlrgasto','placa', 'modeloveiculo', \n",
    "                        'tpveiculo', 'idadeveiculo', 'coordenadas', \n",
    "                        'naturezasinistro', 'dtevento', 'hrevento'],                 \n",
    "                 error_bad_lines=False)    \n",
    "    listSinistros.append(dfaux)\n",
    "    \n",
    "df1 = pd.concat(listSinistros, axis = 0, ignore_index = True)\n",
    "\n",
    "#listSinistros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variaveis nao utilizadas\n",
    "df1 = df1.drop(['cdprocesso','nrprocesso','mercadoria','coordenadas'], axis=1)\n",
    "\n",
    "# remove registros que sejam AVARIA\n",
    "df1=df1[df1['tpsinistro']!='AVARIA']\n",
    "\n",
    "# trata tipos de dados\n",
    "df1['dthrevento'] = pd.to_datetime(df1['dtevento'] + ' ' + df1['hrevento'],\n",
    "                                   format='%d/%m/%Y %H:%M')\n",
    "df1['dtevento'] =  pd.to_datetime(df1['dtevento'], format='%d/%m/%Y')\n",
    "\n",
    "# cria variaveis adicionais\n",
    "df1['diasemana'] = df1['dtevento'].apply( lambda x: x.dayofweek) # 0 - Segunda até 6 - Domingo\n",
    "df1['nrdiasiniciomes'] = df1['dtevento'].apply( lambda x: x.day) # nr dias desde o inicio do mês\n",
    "\n",
    "#df1.loc[df1['mercadoria'].str.contains('DIVERSOS', \n",
    "#                                       flags=re.IGNORECASE, \n",
    "#                                       regex=True)==True, \n",
    "#        'mercadoria'] = 'Diversos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtem a data e hora do evento mais recente\n",
    "df2 = df1.drop([\n",
    "               'tpsinistro','tpoperacao','txtunknow','vlrgasto','placa',\n",
    "               'modeloveiculo','tpveiculo','idadeveiculo',\n",
    "               'naturezasinistro','dtevento','hrevento','diasemana',\n",
    "                'nrdiasiniciomes'], axis=1)\n",
    "\n",
    "df2 = df2.groupby(['nrviagem'], as_index=False).max()\n",
    "\n",
    "# obtem o valor total gasto por viagem\n",
    "df3 = df1.drop([\n",
    "               'tpsinistro','tpoperacao','txtunknow','placa',\n",
    "               'modeloveiculo','tpveiculo','idadeveiculo',\n",
    "               'naturezasinistro','dtevento','hrevento','dthrevento',\n",
    "                'diasemana','nrdiasiniciomes'], axis=1)\n",
    "\n",
    "df3 = df3.groupby(['nrviagem'], as_index=False).sum()\n",
    "\n",
    "df1 = df1.drop(['vlrgasto'], axis=1)\n",
    "\n",
    "df1 = pd.merge(df1, df2, on=['nrviagem','dthrevento'], how='inner')\n",
    "df1 = pd.merge(df1, df3, on=['nrviagem'], how='inner')\n",
    "\n",
    "df1['turnoevento']=df1['dthrevento'].dt.hour\n",
    "df1['turnoevento']=df1['turnoevento'].apply(set_shift)\n",
    "\n",
    "# remove registros duplicados\n",
    "df1.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "#df2 = df2.drop(['dthrevento'], axis=1)\n",
    "\n",
    "del(df2)\n",
    "del(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unifica dados finais\n",
    "df = pd.merge(df, df1, on=['nrviagem'], how='left')\n",
    "\n",
    "del(df1)\n",
    "\n",
    "dfx=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordena colunas no dataframe\n",
    "df = df[['nrviagem','vlrtotal','urbana','cto','possuisinistro','idade','tempowf',\n",
    "        't__ATC','t__LOC','t__OMN','t__ONX','t__OUTRAS','t__SAS','t__SGH','t__TRC',\n",
    "        'p__1','p__2','p__3','p__4','p__5','p__6','p__7','p__9','p__10','p__11','p__12','p__13',\n",
    "        'p__15','p__18','p__19','p__20','p__95','p__96','p__97','p__98','p__99',\n",
    "        'c__ANAPOLIS','c__APARECIDA DE GOIANIA','c__ARAUCARIA','c__BARRA MANSA','c__BARUERI',\n",
    "        'c__BAURU','c__BELEM','c__BELO HORIZONTE','c__BETIM','c__BLUMENAU','c__BRASILIA',\n",
    "        'c__CAMPINAS','c__CANOAS','c__CARAZINHO','c__CAXIAS DO SUL','c__CONTAGEM','c__CUBATAO',\n",
    "        'c__CUIABA','c__CURITIBA','c__DUQUE DE CAXIAS','c__EMBU DAS ARTES','c__FEIRA DE SANTANA',\n",
    "        'c__FORTALEZA','c__GASPAR','c__GOIANIA','c__GUARULHOS','c__HORTOLANDIA','c__ITAJAI',\n",
    "        'c__JABOATAO DOS GUARARAPES','c__JOINVILLE','c__JUNDIAI','c__LONDRINA','c__LOUVEIRA',\n",
    "        'c__MANAUS','c__NAVEGANTES','c__NOVA SANTA RITA','c__OSASCO','c__OUTRAS','c__PARANAGUA',\n",
    "        'c__PAULINIA','c__PORTO ALEGRE','c__RIBEIRAO PRETO','c__RIO DE JANEIRO','c__SALVADOR',\n",
    "        'c__SAO BERNARDO DO CAMPO','c__SAO JOSE DOS PINHAIS','c__SAO PAULO','c__SERRA',\n",
    "        'c__SIMOES FILHO','c__SUMARE','c__UBERLANDIA','c__VIANA','c__VILA VELHA',\n",
    "        'c__VITORIA DE SANTO ANTAO',\n",
    "        'm__ACUCAR','m__AUTO-PECAS','m__BEBIDAS','m__CALCADOS','m__CARGA FRACIONADA',\n",
    "        'm__CIGARROS','m__COMBUSTIVEL','m__COSMETICOS','m__DEFENSIVO AGRICOLA',\n",
    "        'm__DIVERSOS','m__DIVERSOS PRODUTOS','m__EQUIPAMENTOS ELETRICOS','m__ESPECIFICOS',\n",
    "        'm__ESTIRENO','m__FIOS E CABOS','m__MEDICAMENTOS','m__NESTLE','m__OUTRAS',\n",
    "        'm__PECAS AUTOMOTIVAS','m__POLIESTIRENO','m__POLIETILENO','m__PRODUTO ACABADO',\n",
    "        'm__PRODUTOS QUIMICOS','m__TINTAS E SOLVENTES',\n",
    "        'tpsinistro','nrdiasiniciomeswf','diasemanawf',\n",
    "        'salmoco','sjanta','smadrugada','smanha','snoite','starde']]\n",
    "\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "df['vlrtotal'] = pd.to_numeric(df['vlrtotal'].str.replace(',', '.'))\n",
    "\n",
    "#df['vlrtotal'] = pd.to_numeric(df['vlrtotal'])\n",
    "df['urbana'] = df['urbana'].astype('int64')\n",
    "\n",
    "#df.head()\n",
    "#df['urbana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gera dados de WF\n",
    "df.to_csv('/home/dev/poc/arquivos/model/out/multisat-v6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gera dados de roubo\n",
    "roubo=df[df['tpsinistro']!='ACIDENTE']\n",
    "roubo.loc[roubo['tpsinistro']=='ROUBO','tpsinistro']=1\n",
    "roubo.to_csv('/home/dev/poc/arquivos/model/out/roubo-v6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gera dados de acidente\n",
    "acidente=df[df['tpsinistro']!='ROUBO']\n",
    "acidente.loc[acidente['tpsinistro']=='ACIDENTE','tpsinistro']=1\n",
    "acidente.to_csv('/home/dev/poc/arquivos/model/out/acidente-v6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
